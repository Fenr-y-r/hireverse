{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "participant_id = \"P1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from utils.utils import *\n",
    "\n",
    "VID_FILE_PATH = BASE_DIR + \"/data/raw/videos\"\n",
    "OUTPUT_CSV_FILE = BASE_DIR + \"/data/processed/interview_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745524231.157443 15653525 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745524231.180444 15655553 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745524231.189016 15655553 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "from utils.face_analyzer import FaceAnalyzer\n",
    "\n",
    "face_analyzer = FaceAnalyzer()\n",
    "frames = face_analyzer.get_video_frames_for_participant(\n",
    "    participant_id, VID_FILE_PATH, num_selected_frames=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils.LexicalAnalyser import LexicalAnalyser\n",
    "\n",
    "# AUDIO_FILE_PATH = f\"../../data/raw/audio/trimmed_{participant_id}.wav\"\n",
    "# lexical_analyser = LexicalAnalyser(AUDIO_FILE_PATH)\n",
    "# # Extract all features\n",
    "# lexical_features = lexical_analyser.extract_all_features()\n",
    "\n",
    "# # Print the extracted features\n",
    "# print(lexical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745524235.193691 15655558 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "for frame in frames:\n",
    "    detected_faces_landmarks = face_analyzer.process_image_results(frame.image)\n",
    "    frame.facial_landmarks_obj= face_analyzer.get_largest_face_landmarks_obj(frame.image, detected_faces_landmarks)\n",
    "    if frame.facial_landmarks_obj:\n",
    "        frame.facial_landmarks = frame.facial_landmarks_obj.landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for frame in frames:\n",
    "    if frame.facial_landmarks:\n",
    "        frame.face = face_analyzer.get_face_coordinates(frame.facial_landmarks, frame.image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 10.26it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.08it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  6.99it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 10.13it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 10.85it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.19it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  9.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(frames):\n\u001b[1;32m     15\u001b[0m     face_roi \u001b[38;5;241m=\u001b[39m face_analyzer\u001b[38;5;241m.\u001b[39mget_face_roi_image(frame\u001b[38;5;241m.\u001b[39mimage, frame\u001b[38;5;241m.\u001b[39mface, expand_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     frame\u001b[38;5;241m.\u001b[39msmile \u001b[38;5;241m=\u001b[39m smooth_happiness(\u001b[43mface_analyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_smile_from_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_roi\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Downloads/hireverse/src/utils/face_analyzer.py:79\u001b[0m, in \u001b[0;36mFaceAnalyzer.get_smile_from_frame\u001b[0;34m(self, face_roi)\u001b[0m\n\u001b[1;32m     77\u001b[0m face_roi \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(face_roi, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# TODO: align face for better accuracy\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mface_roi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# happiness_prob = result[0][\"emotion\"][\"happy\"] / 100  # Normalize to [0, 1]\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mround\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Downloads/hireverse/venv/lib/python3.9/site-packages/deepface/DeepFace.py:326\u001b[0m, in \u001b[0;36manalyze\u001b[0;34m(img_path, actions, enforce_detection, detector_backend, align, silent)\u001b[0m\n\u001b[1;32m    324\u001b[0m obj \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# facial attribute analysis\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m pbar \u001b[38;5;241m=\u001b[39m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinding actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    328\u001b[0m     action \u001b[38;5;241m=\u001b[39m actions[index]\n",
      "File \u001b[0;32m~/Downloads/hireverse/venv/lib/python3.9/site-packages/tqdm/std.py:1096\u001b[0m, in \u001b[0;36mtqdm.__init__\u001b[0;34m(self, iterable, desc, total, leave, file, ncols, mininterval, maxinterval, miniters, ascii, disable, unit, unit_scale, dynamic_ncols, smoothing, bar_format, initial, position, postfix, unit_divisor, write_bytes, lock_args, nrows, colour, delay, gui, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_free_pos(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mposition\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gui:\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# Initialize the screen printer\u001b[39;00m\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n",
      "File \u001b[0;32m~/Downloads/hireverse/venv/lib/python3.9/site-packages/tqdm/std.py:448\u001b[0m, in \u001b[0;36mtqdm.status_printer\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    446\u001b[0m fp_flush \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m (sys\u001b[38;5;241m.\u001b[39mstderr, sys\u001b[38;5;241m.\u001b[39mstdout):\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflush\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfp_write\u001b[39m(s):\n",
      "File \u001b[0;32m~/Downloads/hireverse/venv/lib/python3.9/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "SMOOTH_WINDOW = 5\n",
    "happiness_buffer = []\n",
    "def smooth_happiness(happiness_prob):\n",
    "    if happiness_prob is None:\n",
    "        return 0 # TODO: change?\n",
    "    happiness_buffer.append(happiness_prob)\n",
    "    if len(happiness_buffer) > SMOOTH_WINDOW:\n",
    "        happiness_buffer.pop(0)\n",
    "    return np.mean(happiness_buffer)\n",
    "\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    face_roi = face_analyzer.get_face_roi_image(frame.image, frame.face, expand_ratio=1.1)\n",
    "    frame.smile = smooth_happiness(face_analyzer.get_smile_from_frame(face_roi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    frame.two_landmarks_connectors = face_analyzer.get_selected_facial_landmarks(frame.facial_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for frame in frames:\n",
    "    result = face_analyzer.get_face_angles(frame.image, frame.facial_landmarks)\n",
    "    frame.face_angles = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosodic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProsodicFeatures(f0_mean=138.8311168142939, f0_min=75.34559605984698, f0_max=599.1469508185926, f0_range=523.8013547587456, f0_sd=67.08530112931376, intensity_mean=-17.989479064941406, intensity_min=-36.43029022216797, intensity_max=0.0, intensity_range=36.43029022216797, intensity_sd=7.597977638244629, f1_mean=626.2173706460609, f1_sd=255.618004717988, f2_mean=1801.0016796933116, f2_sd=369.19335979732216, f3_mean=2718.6018206978147, f3_sd=363.6766279398512, f2_f1_mean=3.2093469178503944, f3_f1_mean=4.813353523730503, f2_f1_sd=1.1984938994193113, f3_f1_sd=1.5116794397601092, jitter=0.02250039669976796, shimmer=0.17853661247111466, percent_unvoiced=14.22418608114985, percent_breaks=1.6075754239154016, max_pause_duration=2.429999999999999, avg_pause_duration=0.31397260273972616, duration=161.134)\n"
     ]
    }
   ],
   "source": [
    "from schemas.model_features import ProsodicFeatures\n",
    "from utils.prosody_analyzer import ProsodyAnalyzer\n",
    " \n",
    "\n",
    "prosody_analyzer = ProsodyAnalyzer(participant_id)\n",
    "prosodic_features: ProsodicFeatures = prosody_analyzer.extract_all_features()\n",
    "print(prosodic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Features Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.feature_storage import FeatureStorage\n",
    "\n",
    "\n",
    "feature_storage = FeatureStorage(OUTPUT_CSV_FILE)\n",
    "facial_features = feature_storage.aggregate_facial_features(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature attributes: {'average_outer_brow_height_mean': 0.015816841919479872, 'average_inner_brow_height_mean': 0.016161303703513142, 'eye_open_mean': 0.011017327997293224, 'outer_lip_height_mean': 0.017899925240511243, 'inner_lip_height_mean': 0.0019792394129144503, 'lip_corner_distance_mean': 0.04586184124940081, 'smile_mean': 34.25866666666667, 'pitch_mean': -5.152441822552064, 'yaw_mean': 5.493754580446394, 'roll_mean': -0.550501627536451, 'average_outer_brow_height_std': 0.0011446686873879937, 'average_inner_brow_height_std': 0.0018430869420889753, 'eye_open_std': 0.001978584199663568, 'outer_lip_height_std': 0.003049972628691677, 'inner_lip_height_std': 0.0013786569838318819, 'lip_corner_distance_std': 0.0030827616358103296, 'smile_std': 24.065104806198818, 'pitch_std': 4.590858943327594, 'yaw_std': 9.416449883562535, 'roll_std': 37.79997069249305, 'average_outer_brow_height_min': 0.013164112545471937, 'average_inner_brow_height_min': 0.012239369718574325, 'eye_open_min': 0.007233642247428866, 'outer_lip_height_min': 0.011499515406211391, 'inner_lip_height_min': 0.00020173421847301888, 'lip_corner_distance_min': 0.039023802597367964, 'smile_min': 0.0, 'pitch_min': -20.499968976667724, 'yaw_min': -18.81418408990875, 'roll_min': -175.9465388781147, 'average_outer_brow_height_max': 0.017624798010086613, 'average_inner_brow_height_max': 0.019373305467744323, 'eye_open_max': 0.013784883879522597, 'outer_lip_height_max': 0.02527721135524996, 'inner_lip_height_max': 0.004598504426177101, 'lip_corner_distance_max': 0.050939246342583774, 'smile_max': 99.0, 'pitch_max': 0.6629979387600133, 'yaw_max': 27.585875333481297, 'roll_max': 16.567100868828653, 'average_outer_brow_height_median': 0.01595562330905452, 'average_inner_brow_height_median': 0.01623459708980883, 'eye_open_median': 0.011760298482048176, 'outer_lip_height_median': 0.017888361794091336, 'inner_lip_height_median': 0.0017487174681580784, 'lip_corner_distance_median': 0.04637843512471491, 'smile_median': 36.4, 'pitch_median': -4.973914962174494, 'yaw_median': 4.415655999555181, 'roll_median': 8.627562284571018}\n",
      "Feature attributes: {'f0_mean': 138.8311168142939, 'f0_min': 75.34559605984698, 'f0_max': 599.1469508185926, 'f0_range': 523.8013547587456, 'f0_sd': 67.08530112931376, 'intensity_mean': -17.98948, 'intensity_min': -36.43029, 'intensity_max': 0.0, 'intensity_range': 36.43029, 'intensity_sd': 7.5979776, 'f1_mean': 626.2173706460609, 'f1_sd': 255.618004717988, 'f2_mean': 1801.0016796933116, 'f2_sd': 369.19335979732216, 'f3_mean': 2718.6018206978147, 'f3_sd': 363.6766279398512, 'f2_f1_mean': 3.2093469178503944, 'f3_f1_mean': 4.813353523730503, 'f2_f1_sd': 1.1984938994193113, 'f3_f1_sd': 1.5116794397601092, 'jitter': 0.02250039669976796, 'shimmer': 0.17853661247111466, 'percent_unvoiced': 14.22418608114985, 'percent_breaks': 1.6075754239154016, 'max_pause_duration': 2.429999999999999, 'avg_pause_duration': 0.31397260273972616, 'duration': 161.134}\n"
     ]
    }
   ],
   "source": [
    "feature_storage.save_to_csv(participant_id, facial_features, prosodic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frame in frames:\n",
    "#     frame.reset_drawable_image()\n",
    "#     # frame.draw_face_border()\n",
    "    \n",
    "#     frame.draw_selected_facial_landmarks(draw_lines=True)\n",
    "    \n",
    "#     frame.put_face_angles()\n",
    "#     # frame.draw_facial_landmarks()\n",
    "#     frame.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
