{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530deee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e3b37",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhireverse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BASE_DIR\n\u001b[1;32m      5\u001b[0m participant_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_preprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_processed_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/raw/videos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparticipant_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.avi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal_projects/hireverse/src/hireverse/pipelines/cnn_preprocessing.py:31\u001b[0m, in \u001b[0;36mget_processed_frames\u001b[0;34m(vid_file_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m frames \u001b[38;5;241m=\u001b[39m filtered_frames\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     frame\u001b[38;5;241m.\u001b[39malign_face_with_mediapipe_landmarks()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mfacial_landmarks:\n",
      "File \u001b[0;32m~/personal_projects/hireverse/src/hireverse/schemas/frame.py:158\u001b[0m, in \u001b[0;36mFrame.resize\u001b[0;34m(self, new_width, new_height)\u001b[0m\n\u001b[1;32m    155\u001b[0m     aspect_ratio \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m/\u001b[39m width\n\u001b[1;32m    156\u001b[0m     new_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(new_width \u001b[38;5;241m*\u001b[39m aspect_ratio)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_height\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_CUBIC\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hireverse.pipelines.cnn_preprocessing as cnn_preprocessing\n",
    "from hireverse.utils.utils import BASE_DIR\n",
    "\n",
    "participant_id = \"P4\"\n",
    "frames = cnn_preprocessing.get_processed_frames(os.path.join(BASE_DIR, \"data/raw/videos\", f\"{participant_id}.avi\"), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d23b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 638, 638, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 319, 319, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 319, 319, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 319, 319, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 317, 317, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 158, 158, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 158, 158, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 158, 158, 64)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 156, 156, 2048)    1181696   \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 2048)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 600)               1229400   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2430296 (9.27 MB)\n",
      "Trainable params: 2430104 (9.27 MB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_competency_cnn(input_shape=(640, 640, 1), num_classes=600):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Initial Convolutional Blocks\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    # Final Convolutional Layers\n",
    "    model.add(layers.Conv2D(2048, (3, 3), activation='relu'))\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Final Classification Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # 6 competencies Ã— 100 classes\n",
    "\n",
    "    # Compile with learning rate (as per paper)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = build_competency_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c895b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Prepare Data\n",
    "X = np.array([frame.image for frame in frames])  # Convert frames to numpy array\n",
    "y = np.array(turker_scores_full_interview)      # Your labels\n",
    "\n",
    "# Add channel dimension if grayscale\n",
    "if X.ndim == 3:\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Create Data Generators\n",
    "def create_data_generator(images, labels, batch_size=32, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(images))\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = create_data_generator(X_train, y_train, batch_size)\n",
    "val_dataset = create_data_generator(X_val, y_val, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build and Train Model\n",
    "model = build_competency_cnn()\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71feeff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build and Train Model\n",
    "model = build_competency_cnn()\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
